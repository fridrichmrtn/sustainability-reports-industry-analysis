{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class DataExploration():\n",
    "    \n",
    "    def __init__(self, data_path = \"../../data/processed.parquet\"):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_path)\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "### prepare\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import pycld2 as cld2\n",
    "import spacy\n",
    "import gc\n",
    "\n",
    "class DataProcessing():\n",
    "    \n",
    "    def __init__(self,\n",
    "                data_file = \"../../data/ingested.parquet\",\n",
    "                data_folder = \"../../data/\") -> None:\n",
    "        self.data_file = data_file\n",
    "        self.data_folder = data_folder\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_file)\n",
    "        self.data = self.data.iloc[:100,:]\n",
    "        return self\n",
    "    \n",
    "    def _load_text(self, file_path):\n",
    "        if file_path is None:\n",
    "            return None\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>|</.*?>\",\"\", text)\n",
    "        text = re.sub(r\"(s?)(f|ht)tp(s?)://\\S+\\b\",\"\", text)\n",
    "        text = re.sub(r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\",\"\", text) #email\n",
    "        text = re.sub(r\"\\\\-\",\"\", text)\n",
    "        text = re.sub(\"[^a-z '.,?!:]\",\" \", text)\n",
    "        text = re.sub(r\"\\b(\\w+\\s*)\\1{1,}\", \" \", text) #dupli \"\\\\1\"\n",
    "        return re.sub(r\" +\",\" \", text)\n",
    "    \n",
    "    def _preprocess_row(self, ind):\n",
    "        # preprocess\n",
    "        row = self.data.loc[ind].copy()\n",
    "        #row[\"raw_text\"] = self._load_text(row[\"txt_file_destination\"])\n",
    "        row[\"text\"] = self._preprocess_text(\n",
    "            self._load_text(row[\"txt_file_destination\"]))\n",
    "        return row\n",
    "        \n",
    "    def preprocess_reports(self, n_jobs = 8):\n",
    "        self.data = self.data.loc[(self.data.txt_file_destination.notnull()),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._preprocess_row)\\\n",
    "            (ind) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def _check_path(self,path):\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    def _get_parquet_files(self, dir_name):\n",
    "        for file in os.listdir(dir_name):\n",
    "            yield pd.read_parquet(os.path.join(dir_name, file))        \n",
    "\n",
    "    def _get_batches(self, data, batch_size = 64):\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            yield data.iloc[i:i+batch_size,:]\n",
    "\n",
    "    def _deconstruct_upos_batch(self, data, col = \"text\", n_process = 1, batch_size = 1):\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        nlp.max_length = 20000000\n",
    "        docs = nlp.pipe(data.loc[:,col].values,\n",
    "            n_process=n_process, batch_size=batch_size)\n",
    "        parsed_ls = [(data.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "            t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]\n",
    "        return pd.DataFrame(parsed_ls,\n",
    "            columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "                \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "\n",
    "    def _deconstruct_save_upos_batch(self, data, dir_name, col = \"text\", n_jobs = 1, batch_size = 1):\n",
    "        batches = self._get_batches(data)\n",
    "        for i, v in enumerate(batches):\n",
    "            upos = self._deconstruct_upos_batch(data, col, n_jobs, batch_size)\n",
    "            upos.to_parquet(dir_name+str(i)+\"_batch.parquet\")\n",
    "            del upos\n",
    "            gc.collect()\n",
    "        return None\n",
    "    \n",
    "    def _filter_upos(self, upos):\n",
    "        # univariate filter\n",
    "        upos = upos.loc[upos.pos.isin([\"NOUN\", \"ADJ\", \"VERB\"]),:] \n",
    "        upos = upos.loc[~upos.is_stopword,:]\n",
    "        upos = upos.loc[(upos.lemma.str.len()>2) & (upos.lemma.str.len()<19),:]\n",
    "        # multivariate filter\n",
    "        lemma_stats = upos.groupby(\"lemma\", as_index=False).agg({\"doc_id\":[\"count\", \"nunique\"]})\n",
    "        pf = (lemma_stats[(\"doc_id\",\"count\")]>10)&(lemma_stats[(\"doc_id\",\"nunique\")]>5) #500,250\n",
    "        stopword_set = set([])\n",
    "        lemma_set = set(lemma_stats.loc[pf,\"lemma\"].values).difference(stopword_set)\n",
    "        return upos.loc[upos.lemma.isin(lemma_set),:]\n",
    "\n",
    "    def _reconstruct_upos(self, upos, col = \"reconstructed_text\"):\n",
    "        # reconstruct text\n",
    "        reconstructed = pd.DataFrame(upos.groupby(\"doc_id\")\\\n",
    "            .apply(lambda x:\" \".join(x[\"lemma\"])), columns=[col])\n",
    "        # clean up\n",
    "        reconstructed[col] = reconstructed[col].apply(\\\n",
    "            lambda x: re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', x))  \n",
    "        return reconstructed\n",
    "\n",
    "    def construct_upos(self, n_jobs = 8, batch_size=2,\n",
    "        dir_name = \"../../data/upos_files/\", col = \"text\"):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        # deconstruct in parallel and save\n",
    "        self._deconstruct_save_upos_batch(self.data, dir_name, col, n_jobs, batch_size)\n",
    "        # load back\n",
    "        upos = pd.concat(self._get_parquet_files(dir_name)) \n",
    "        # filter\n",
    "        upos = self._filter_upos(upos)\n",
    "        # reconstruct text and merge back\n",
    "        self.data = self.data.merge(self._reconstruct_upos(upos),\n",
    "            how=\"inner\", left_index=True, right_index=True)   \n",
    "        return self\n",
    "    \n",
    "    def _metadata_row(self, ind, col = \"reconstructed_text\"):\n",
    "        row = self.data.loc[ind].copy()\n",
    "        row[\"n_chars\"] = len(row[col])\n",
    "        row[\"n_words\"] = len(re.split(\"\\w+\",row[col]))\n",
    "        row[\"n_sentences\"] = len(re.split(r\"[.?!]\", row[col]))\n",
    "\n",
    "        lang_estimation = cld2.detect(row[col], returnVectors=True)[2]\n",
    "        row[\"language\"] = lang_estimation[0][1]\n",
    "        row[\"language_score\"] = lang_estimation[0][2]/100.0\n",
    "        return row    \n",
    "    \n",
    "    def get_metadata(self, col = \"reconstructed_text\", n_jobs = 4):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._metadata_row)\\\n",
    "            (ind, col) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def save_data(self, file_path = None):\n",
    "        if file_path is None:\n",
    "            file_path = self.data_folder+\"processed.parquet\"\n",
    "        self.data.to_parquet(file_path)\n",
    "        return self    \n",
    "\n",
    "# %%\n",
    "Processing = DataProcessing().load_data().preprocess_reports()\\\n",
    "    .construct_upos()#.get_metadata().save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = Processing.data\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 20000000\n",
    "parsed = nlp.pipe(docs.loc[:,\"text\"].values, batch_size=10, n_process=8)\n",
    "parsed_ls = [(docs.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "    t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = _deconstruct_upos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(df, batch_size = 80):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size,:]\n",
    "\n",
    "def _deconstruct_upos(df, col = \"text\", n_process = 8, batch_size = 10):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    nlp.max_length = 20000000\n",
    "    docs = nlp.pipe(df.loc[:,col].values,\n",
    "        n_process=n_process, batch_size=batch_size)\n",
    "    parsed_ls = [(df.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "        t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]\n",
    "    return pd.DataFrame(parsed_ls,\n",
    "        columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "            \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "\n",
    "data = Processing.data\n",
    "\n",
    "data_batch = get_batch_data(data)\n",
    "for i, v in enumerate(data_batch):\n",
    "    upos = _deconstruct_upos(v)\n",
    "    upos.to_parquet(\"../../data/upos_files/\"+str(i)+\"_batch.parquet\")\n",
    "    del upos\n",
    "upos = pd.concat(get_batch_files())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_files(dir_name = \"../../data/upos_files/\"):\n",
    "    for file in os.listdir(dir_name):\n",
    "        yield pd.read_parquet(os.path.join(dir_name, file))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
