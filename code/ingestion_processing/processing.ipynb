{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class DataExploration():\n",
    "    \n",
    "    def __init__(self, data_path = \"../../data/processed.parquet\"):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_path)\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dada = pd.read_parquet(\"../../data/processed.parquet\")\n",
    "#dada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 162\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m    \n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m    161\u001b[0m Processing \u001b[38;5;241m=\u001b[39m \u001b[43mDataProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m--> 162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_upos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_metadata()\u001b[38;5;241m.\u001b[39msave_data()\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mDataProcessing.construct_upos\u001b[0;34m(self, n_jobs, dir_name, col)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[:,col]\u001b[38;5;241m.\u001b[39mnotnull())\\\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mloc[:,col]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])),]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# deconstruct and save\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m rows_ls \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_desconstruct_save_upos_row\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows_ls) \u001b[38;5;66;03m# NOTE: filter cols\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m rows_ls\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m, in \u001b[0;36mDataProcessing._desconstruct_save_upos_row\u001b[0;34m(self, ind, dir_name, overwrite, col)\u001b[0m\n\u001b[1;32m     82\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupos_file_destination\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m file_path\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     upos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deconstruct_upos_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     upos\u001b[38;5;241m.\u001b[39mto_parquet(file_path)\n\u001b[1;32m     86\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupos_file_destination\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_path(file_path)\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mDataProcessing._deconstruct_upos_row\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m     69\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m nlp\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000000\u001b[39m\n\u001b[0;32m---> 71\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m parsed_ls \u001b[38;5;241m=\u001b[39m [(row\u001b[38;5;241m.\u001b[39mname, t\u001b[38;5;241m.\u001b[39mtext, t\u001b[38;5;241m.\u001b[39mlemma_, t\u001b[38;5;241m.\u001b[39mpos_, t\u001b[38;5;241m.\u001b[39mtag_, t\u001b[38;5;241m.\u001b[39mdep_,\n\u001b[1;32m     73\u001b[0m     t\u001b[38;5;241m.\u001b[39mshape_, t\u001b[38;5;241m.\u001b[39mis_alpha, t\u001b[38;5;241m.\u001b[39mis_stop) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m parsed]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(parsed_ls,\n\u001b[1;32m     75\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_stopword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:53\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/pipeline/transition_parser.pyx:343\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/pipeline/_parser_internals/ner.pyx:275\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/tokens/doc.pyx:811\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/Data/git_root/sustainability-reports-industry-analysis/.env/lib/python3.10/site-packages/spacy/tokens/doc.pyx:127\u001b[0m, in \u001b[0;36mspacy.tokens.doc.SetEntsDefault.values\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:451\u001b[0m, in \u001b[0;36mEnumMeta.__members__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_names_)\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__members__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Returns a mapping of member name->value.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    This mapping lists all enum members, including aliases. Note that this\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    is a read-only view of the internal mapping.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MappingProxyType(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_map_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "### prepare\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import pycld2 as cld2\n",
    "import spacy\n",
    "\n",
    "class DataProcessing():\n",
    "    \n",
    "    def __init__(self,\n",
    "                data_file = \"../../data/ingested.parquet\",\n",
    "                data_folder = \"../../data/\") -> None:\n",
    "        self.data_file = data_file\n",
    "        self.data_folder = data_folder\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_file)\n",
    "        self.data = self.data.iloc[:1500,:]\n",
    "        return self\n",
    "    \n",
    "    def _load_text(self, file_path):\n",
    "        if file_path is None:\n",
    "            return None\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>|</.*?>\",\"\", text)\n",
    "        text = re.sub(r\"(s?)(f|ht)tp(s?)://\\S+\\b\",\"\", text)\n",
    "        text = re.sub(r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\",\"\", text) #email\n",
    "        text = re.sub(r\"\\\\-\",\"\", text)\n",
    "        text = re.sub(\"[^a-z '.,?!:]\",\" \", text)\n",
    "        text = re.sub(r\"\\b(\\w+\\s*)\\1{1,}\", \" \", text) #dupli \"\\\\1\"\n",
    "        return re.sub(r\" +\",\" \", text)\n",
    "    \n",
    "    def _preprocess_row(self, ind):\n",
    "        # preprocess\n",
    "        row = self.data.loc[ind].copy()\n",
    "        #row[\"raw_text\"] = self._load_text(row[\"txt_file_destination\"])\n",
    "        row[\"text\"] = self._preprocess_text(\n",
    "            self._load_text(row[\"txt_file_destination\"]))\n",
    "        return row\n",
    "        \n",
    "    def preprocess_reports(self, n_jobs = 8):\n",
    "        self.data = self.data.loc[(self.data.txt_file_destination.notnull()),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._preprocess_row)\\\n",
    "            (ind) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def _check_path(self,path):\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    def _get_upos_path(self, file_path,\n",
    "            dir_name=\"../../data/txt_files/\"):\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name, extension = os.path.splitext(base_name)\n",
    "        return os.path.join(*[dir_name, name+\".parquet\"])\n",
    "                \n",
    "    def _deconstruct_upos_row(self, row, col = \"text\"):\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        nlp.max_length = 20000000\n",
    "        parsed = nlp(row[col])\n",
    "        parsed_ls = [(row.name, t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "            t.shape_, t.is_alpha, t.is_stop) for t in parsed]\n",
    "        return pd.DataFrame(parsed_ls,\n",
    "            columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "                \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "    \n",
    "    def _desconstruct_save_upos_row(self, ind, dir_name, overwrite=False, col = \"text\"):\n",
    "        row = self.data.loc[ind,:].copy()\n",
    "        file_path = self._get_upos_path(row[\"txt_file_destination\"], dir_name)\n",
    "        if overwrite is False and self._check_path(file_path) is not None:\n",
    "            row[\"upos_file_destination\"] = file_path\n",
    "        else:\n",
    "            upos = self._deconstruct_upos_row(row, col)\n",
    "            upos.to_parquet(file_path)\n",
    "            row[\"upos_file_destination\"] = self._check_path(file_path)\n",
    "        return row\n",
    "    \n",
    "    def _load_upos_row(self, ind):\n",
    "        if self._check_path(self.data.loc[ind,\"upos_file_destination\"]) is not None:\n",
    "            return pd.read_parquet(self.data.loc[ind,\"upos_file_destination\"])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _filter_upos(self, upos):\n",
    "        # univariate filter\n",
    "        upos = upos.loc[upos.pos.isin([\"NOUN\", \"ADJ\", \"VERB\"]),:] \n",
    "        upos = upos.loc[~upos.is_stopword,:]\n",
    "        upos = upos.loc[(upos.lemma.str.len()>2) & (upos.lemma.str.len()<19),:]\n",
    "        # multivariate filter\n",
    "        lemma_stats = upos.groupby(\"lemma\", as_index=False).agg({\"doc_id\":[\"count\", \"nunique\"]})\n",
    "        pf = (lemma_stats[(\"doc_id\",\"count\")]>1000)&(lemma_stats[(\"doc_id\",\"nunique\")]>500) #500,250\n",
    "        stopword_set = set([])\n",
    "        lemma_set = set(lemma_stats.loc[pf,\"lemma\"].values).difference(stopword_set)\n",
    "        return upos.loc[upos.lemma.isin(lemma_set),:]\n",
    "\n",
    "    def _reconstruct_upos(self, upos, col = \"reconstructed_text\"):\n",
    "        # reconstruct text\n",
    "        reconstructed = pd.DataFrame(upos.groupby(\"doc_id\")\\\n",
    "            .apply(lambda x:\" \".join(x[\"lemma\"])), columns=[col])\n",
    "        # clean up\n",
    "        reconstructed[col] = reconstructed[col].apply(\\\n",
    "            lambda x: re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', x))  \n",
    "        return reconstructed\n",
    "\n",
    "    def construct_upos(self, n_jobs = 4, dir_name = \"../../data/upos_files/\", col = \"text\"):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        # deconstruct and save\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)\\\n",
    "                (delayed(self._desconstruct_save_upos_row)\\\n",
    "            (ind, dir_name, False) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls) # NOTE: filter cols\n",
    "        del rows_ls\n",
    "        # load upos\n",
    "        upos_ls = [self._load_upos_row(ind) for ind in self.data.index]\n",
    "        upos = pd.concat(upos_ls)\n",
    "        del upos_ls\n",
    "        upos = self._filter_upos(upos)\n",
    "        # reconstruct text and merge back\n",
    "        self.data = self.data.merge(self._reconstruct_upos(upos),\n",
    "            how=\"inner\", left_index=True, right_index=True)   \n",
    "        return self\n",
    "    \n",
    "    def _metadata_row(self, ind, col = \"reconstructed_text\"):\n",
    "        row = self.data.loc[ind].copy()\n",
    "        row[\"n_chars\"] = len(row[col])\n",
    "        row[\"n_words\"] = len(re.split(\"\\w+\",row[col]))\n",
    "        row[\"n_sentences\"] = len(re.split(r\"[.?!]\", row[col]))\n",
    "\n",
    "        lang_estimation = cld2.detect(row[col], returnVectors=True)[2]\n",
    "        row[\"language\"] = lang_estimation[0][1]\n",
    "        row[\"language_score\"] = lang_estimation[0][2]/100.0\n",
    "        return row    \n",
    "    \n",
    "    def get_metadata(self, col = \"reconstructed_text\", n_jobs = 4):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._metadata_row)\\\n",
    "            (ind, col) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def save_data(self, file_path = None):\n",
    "        if file_path is None:\n",
    "            file_path = self.data_folder+\"processed.parquet\"\n",
    "        self.data.to_parquet(file_path)\n",
    "        return self    \n",
    "\n",
    "# %%\n",
    "Processing = DataProcessing().load_data().preprocess_reports()\\\n",
    "    .construct_upos().get_metadata().save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_ls = Parallel(n_jobs=8)(delayed(self._preprocess_row)(ind) for ind in self.data.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
