{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class DataExploration():\n",
    "    \n",
    "    def __init__(self, data_path = \"../../data/processed.parquet\"):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_path)\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dada = pd.read_parquet(\"../../data/processed.parquet\")\n",
    "#dada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "### prepare\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import pycld2 as cld2\n",
    "import spacy\n",
    "\n",
    "class DataProcessing():\n",
    "    \n",
    "    def __init__(self,\n",
    "                data_file = \"../../data/ingested.parquet\",\n",
    "                data_folder = \"../../data/\") -> None:\n",
    "        self.data_file = data_file\n",
    "        self.data_folder = data_folder\n",
    "        pass\n",
    "\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_parquet(self.data_file)\n",
    "        self.data = self.data.iloc[1000:1100,:]\n",
    "        return self\n",
    "    \n",
    "    def _load_text(self, file_path):\n",
    "        if file_path is None:\n",
    "            return None\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>|</.*?>\",\"\", text)\n",
    "        text = re.sub(r\"(s?)(f|ht)tp(s?)://\\S+\\b\",\"\", text)\n",
    "        text = re.sub(r\"^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$\",\"\", text) #email\n",
    "        text = re.sub(r\"\\\\-\",\"\", text)\n",
    "        text = re.sub(\"[^a-z '.,?!:]\",\" \", text)\n",
    "        text = re.sub(r\"\\b(\\w+\\s*)\\1{1,}\", \" \", text) #dupli \"\\\\1\"\n",
    "        return re.sub(r\" +\",\" \", text)\n",
    "    \n",
    "    def _preprocess_row(self, ind):\n",
    "        # preprocess\n",
    "        row = self.data.loc[ind].copy()\n",
    "        #row[\"raw_text\"] = self._load_text(row[\"txt_file_destination\"])\n",
    "        row[\"text\"] = self._preprocess_text(\n",
    "            self._load_text(row[\"txt_file_destination\"]))\n",
    "        return row\n",
    "        \n",
    "    def preprocess_reports(self, n_jobs = 8):\n",
    "        self.data = self.data.loc[(self.data.txt_file_destination.notnull()),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._preprocess_row)\\\n",
    "            (ind) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def _check_path(self,path):\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            return None    \n",
    "\n",
    "    def _get_upos_path(self, file_path,\n",
    "            dir_name=\"../../data/txt_files/\"):\n",
    "        base_name = os.path.basename(file_path)\n",
    "        name, extension = os.path.splitext(base_name)\n",
    "        return os.path.join(*[dir_name, name+\".parquet\"])\n",
    "                \n",
    "    def _deconstruct_upos_row(self, row, col = \"text\"):\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        nlp.max_length = 20000000\n",
    "        parsed = nlp(row[col])\n",
    "        parsed_ls = [(row.name, t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "            t.shape_, t.is_alpha, t.is_stop) for t in parsed]\n",
    "        return pd.DataFrame(parsed_ls,\n",
    "            columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "                \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "    \n",
    "    def _desconstruct_save_upos_row(self, ind, dir_name, overwrite=False, col = \"text\"):\n",
    "        row = self.data.loc[ind,:].copy()\n",
    "        file_path = self._get_upos_path(row[\"txt_file_destination\"], dir_name)\n",
    "        if overwrite is False and self._check_path(file_path) is not None:\n",
    "            row[\"upos_file_destination\"] = file_path\n",
    "        else:\n",
    "            upos = self._deconstruct_upos_row(row, col)\n",
    "            upos.to_parquet(file_path)\n",
    "            row[\"upos_file_destination\"] = self._check_path(file_path)\n",
    "        return row\n",
    "    \n",
    "    def _load_upos_row(self, ind):\n",
    "        if self._check_path(self.data.loc[ind,\"upos_file_destination\"]) is not None:\n",
    "            return pd.read_parquet(self.data.loc[ind,\"upos_file_destination\"])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _filter_upos(self, upos):\n",
    "        # univariate filter\n",
    "        upos = upos.loc[upos.pos.isin([\"NOUN\", \"ADJ\", \"VERB\"]),:] \n",
    "        upos = upos.loc[~upos.is_stopword,:]\n",
    "        upos = upos.loc[(upos.lemma.str.len()>2) & (upos.lemma.str.len()<19),:]\n",
    "        # multivariate filter\n",
    "        lemma_stats = upos.groupby(\"lemma\", as_index=False).agg({\"doc_id\":[\"count\", \"nunique\"]})\n",
    "        pf = (lemma_stats[(\"doc_id\",\"count\")]>1000)&(lemma_stats[(\"doc_id\",\"nunique\")]>500) #500,250\n",
    "        stopword_set = set([])\n",
    "        lemma_set = set(lemma_stats.loc[pf,\"lemma\"].values).difference(stopword_set)\n",
    "        return upos.loc[upos.lemma.isin(lemma_set),:]\n",
    "\n",
    "    def _reconstruct_upos(self, upos, col = \"reconstructed_text\"):\n",
    "        # reconstruct text\n",
    "        reconstructed = pd.DataFrame(upos.groupby(\"doc_id\")\\\n",
    "            .apply(lambda x:\" \".join(x[\"lemma\"])), columns=[col])\n",
    "        # clean up\n",
    "        reconstructed[col] = reconstructed[col].apply(\\\n",
    "            lambda x: re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', x))  \n",
    "        return reconstructed\n",
    "\n",
    "    def construct_upos(self, n_jobs = 4, dir_name = \"../../data/upos_files2/\", col = \"text\"):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        # deconstruct and save\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)\\\n",
    "                (delayed(self._desconstruct_save_upos_row)\\\n",
    "            (ind, dir_name, False) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls) # NOTE: filter cols\n",
    "        del rows_ls\n",
    "        \n",
    "        # load upos\n",
    "        upos_ls = [self._load_upos_row(ind) for ind in self.data.index]\n",
    "        upos = pd.concat(upos_ls)\n",
    "        del upos_ls\n",
    "        upos = self._filter_upos(upos)\n",
    "        # reconstruct text and merge back\n",
    "        self.data = self.data.merge(self._reconstruct_upos(upos),\n",
    "            how=\"inner\", left_index=True, right_index=True)   \n",
    "        return self\n",
    "    \n",
    "    def _metadata_row(self, ind, col = \"reconstructed_text\"):\n",
    "        row = self.data.loc[ind].copy()\n",
    "        row[\"n_chars\"] = len(row[col])\n",
    "        row[\"n_words\"] = len(re.split(\"\\w+\",row[col]))\n",
    "        row[\"n_sentences\"] = len(re.split(r\"[.?!]\", row[col]))\n",
    "\n",
    "        lang_estimation = cld2.detect(row[col], returnVectors=True)[2]\n",
    "        row[\"language\"] = lang_estimation[0][1]\n",
    "        row[\"language_score\"] = lang_estimation[0][2]/100.0\n",
    "        return row    \n",
    "    \n",
    "    def get_metadata(self, col = \"reconstructed_text\", n_jobs = 4):\n",
    "        self.data = self.data.loc[(self.data.loc[:,col].notnull())\\\n",
    "            & (~self.data.loc[:,col].isin([\"\"])),]\n",
    "        rows_ls = Parallel(n_jobs = n_jobs)(delayed(self._metadata_row)\\\n",
    "            (ind, col) for ind in self.data.index)\n",
    "        self.data = pd.DataFrame(rows_ls)\n",
    "        return self\n",
    "    \n",
    "    def save_data(self, file_path = None):\n",
    "        if file_path is None:\n",
    "            file_path = self.data_folder+\"processed.parquet\"\n",
    "        self.data.to_parquet(file_path)\n",
    "        return self    \n",
    "\n",
    "# %%\n",
    "Processing = DataProcessing().load_data().preprocess_reports()#\\\n",
    "    #.construct_upos().get_metadata().save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000000\u001b[39m\n\u001b[1;32m      4\u001b[0m parsed \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(docs\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m parsed_ls \u001b[38;5;241m=\u001b[39m [(docs\u001b[38;5;241m.\u001b[39mindex[i], t\u001b[38;5;241m.\u001b[39mtext, t\u001b[38;5;241m.\u001b[39mlemma_, t\u001b[38;5;241m.\u001b[39mpos_, t\u001b[38;5;241m.\u001b[39mtag_, t\u001b[38;5;241m.\u001b[39mdep_,\n\u001b[1;32m      6\u001b[0m     t\u001b[38;5;241m.\u001b[39mshape_, t\u001b[38;5;241m.\u001b[39mis_alpha, t\u001b[38;5;241m.\u001b[39mis_stop) \u001b[38;5;28;01mfor\u001b[39;00m i, parsed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m parsed]\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m nlp\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000000\u001b[39m\n\u001b[1;32m      4\u001b[0m parsed \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mpipe(docs\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m parsed_ls \u001b[38;5;241m=\u001b[39m [(docs\u001b[38;5;241m.\u001b[39mindex[i], \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, t\u001b[38;5;241m.\u001b[39mlemma_, t\u001b[38;5;241m.\u001b[39mpos_, t\u001b[38;5;241m.\u001b[39mtag_, t\u001b[38;5;241m.\u001b[39mdep_,\n\u001b[1;32m      6\u001b[0m     t\u001b[38;5;241m.\u001b[39mshape_, t\u001b[38;5;241m.\u001b[39mis_alpha, t\u001b[38;5;241m.\u001b[39mis_stop) \u001b[38;5;28;01mfor\u001b[39;00m i, parsed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m parsed]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "docs = Processing.data\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.max_length = 20000000\n",
    "parsed = nlp.pipe(docs.loc[:,\"text\"].values, batch_size=10, n_process=8)\n",
    "parsed_ls = [(docs.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "    t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_files(self, dir_name):\n",
    "    for file in os.listdir(dir_name):\n",
    "        yield pd.read_parquet(os.path.join(dir_name, file))        \n",
    "\n",
    "def _get_batch(self, data, batch_size = 80):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield data.iloc[i:i+batch_size,:]\n",
    "\n",
    "def _deconstruct_upos_batch(self, data, col = \"text\", n_process = 8, batch_size = 10):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    nlp.max_length = 20000000\n",
    "    docs = nlp.pipe(data.loc[:,col].values,\n",
    "        n_process=n_process, batch_size=batch_size)\n",
    "    parsed_ls = [(data.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "        t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]\n",
    "    return pd.DataFrame(parsed_ls,\n",
    "        columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "            \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "\n",
    "def _deconstruct_save_upos_batch(self, data, dir_name, col = \"text\", n_process = 8, batch_size = 10):\n",
    "    data_batch = self._get_batch_data(data)\n",
    "    for i, v in enumerate(data_batch):\n",
    "        upos = self._deconstruct_upos_batch(data, col, n_process, batch_size)\n",
    "        upos.to_parquet(dir_name+str(i)+\"_batch.parquet\")\n",
    "        del upos\n",
    "    return None\n",
    "\n",
    "def _recoconstruct_upos(self, dir_name, col = \"reconstructed_text\"):\n",
    "    # reconstruct text\n",
    "    upos = pd.concat(self._get_files(dir_name)) \n",
    "    reconstructed = pd.DataFrame(upos.groupby(\"doc_id\")\\\n",
    "        .apply(lambda x:\" \".join(x[\"lemma\"])), columns=[col])\n",
    "    # clean up\n",
    "    reconstructed[col] = reconstructed[col].apply(\\\n",
    "        lambda x: re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', x))  \n",
    "    return reconstructed\n",
    "\n",
    "#df = _deconstruct_upos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = Processing.data\n",
    "data_bach = get_batch(data)\n",
    "\n",
    "ls = []\n",
    "while True:\n",
    "    try:\n",
    "        ls.append(_deconstruct_upos(next(df)))\n",
    "    except StopIteration:\n",
    "        break\n",
    "df0 = pd.concat(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.doc_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(df, batch_size = 80):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size,:]\n",
    "\n",
    "def _deconstruct_upos(df, col = \"text\", n_process = 8, batch_size = 10):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    nlp.max_length = 20000000\n",
    "    docs = nlp.pipe(df.loc[:,col].values,\n",
    "        n_process=n_process, batch_size=batch_size)\n",
    "    parsed_ls = [(df.index[i], t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "        t.shape_, t.is_alpha, t.is_stop) for i, parsed in enumerate(docs) for t in parsed]\n",
    "    return pd.DataFrame(parsed_ls,\n",
    "        columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "            \"dep\", \"shape\", \"is_alpha\", \"is_stopword\"])\n",
    "\n",
    "data = Processing.data\n",
    "\n",
    "data_batch = get_batch_data(data)\n",
    "for i, v in enumerate(data_batch):\n",
    "    upos = _deconstruct_upos(v)\n",
    "    upos.to_parquet(\"../../data/upos_files/\"+str(i)+\"_batch.parquet\")\n",
    "    del upos\n",
    "upos = pd.concat(get_batch_files())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_files(dir_name = \"../../data/upos_files/\"):\n",
    "    for file in os.listdir(dir_name):\n",
    "        yield pd.read_parquet(os.path.join(dir_name, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
